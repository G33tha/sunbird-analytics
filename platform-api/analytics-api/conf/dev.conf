spark.cassandra.connection.host=172.31.25.29
service.search.url="http://172.31.24.39:9000"
service.search.path="/v2/search"
service.search.requestbody="""{"request":{"filters":{"objectType":["Content"],"contentType":["Story","Worksheet","Collection","Game"],"status":["Live"]},"limit":1000}}"""
service.search.limit="10"
recommendation.enable=true


content2vec.content_service_url="http://172.31.24.39:8080/learning-service"
content2vec.scripts_path="/mnt/data/analytics/scripts/vidyavaani"
content2vec.s3_bucket="sandbox-data-store"
content2vec.s3_key_prefix="content2vec/model/"
content2vec.model_path="/mnt/data/analytics/content2vec/model/"
content2vec.kafka_topic="sandbox.learning.graph.events"
content2vec.kafka_broker_list="172.31.1.92:9092"
content2vec.content_corpus="true"
content2vec.enrich_content="true"
content2vec.train_model="true"
content2vec.infer_query="true"
content2vec.infer_all="false"
content2vec.corpus_path="/mnt/data/analytics/content2vec/content_corpus/"
content2vec.download_path="/mnt/data/analytics/content2vec/download/"
content2vec.download_file_prefix="temp_"
content2vec.train_model_job="/mnt/data/analytics/scripts/run-job.sh ctv &"
recommendation.train_model_job="/mnt/data/analytics/scripts/run-job.sh device-recos &"
dataproduct.scripts_path="/mnt/data/analytics/scripts"

# Metrics API configuration
metrics.search.type="s3"
metrics.search.params={"bucket":"sandbox-data-store", "path":"metrics/"}
metrics.period.format.day="MMM dd EEE"
metrics.period.format.month="MMM YYYY"
metrics.period.format.year="YYYY"


# Data Exhaust API
dataExhaust.job.brokerList="172.31.1.92:9092"
dataExhaust.job.topic="sandbox.analytics.job_queue"
dataExhaust.data.brokerList="172.31.1.92:9092"
dataExhaust.data.topic="sandbox.telemetry.derived" 
data_exhaust.s3_bucket="data_exhaust"
data_exhaust.jobs.list.limit=100

 # Log4j Kafka appender config
log4j.appender.kafka.enable="false"
log4j.appender.kafka.broker_host="172.31.1.92:9092"
log4j.appender.kafka.topic="sandbox.telemetry.backend"

#AKKA Configuration
akka {
  actor {
  	deployment {

        /metricsApiActor {
            router = round-robin-pool
            nr-of-instances = 10
        }
        /jobApiActor {
            router = round-robin-pool
            nr-of-instances = 10
        }
    }
  	
  	default-dispatcher {
	    executor = "fork-join-executor"
	  	fork-join-executor {
		  # The parallelism factor is used to determine thread pool size using the
		  # following formula: ceil(available processors * factor). Resulting size
		  # is then bounded by the parallelism-min and parallelism-max values.
		  parallelism-factor = 3.0
	
		  # Min number of threads to cap factor-based parallelism number to
		  parallelism-min = 8
	
		   # Max number of threads to cap factor-based parallelism number to
		   parallelism-max = 64
		 }
		 # Throughput for default Dispatcher, set to 1 for as fair as possible
	     throughput = 1
	  }
  }
}

#Netty Configuration
play.server {

  # The server provider class name
  provider = "play.core.server.NettyServerProvider"

  netty {

    # The number of event loop threads. 0 means let Netty decide, which by default will select 2 times the number of
    # available processors.
    eventLoopThreads = 30

    # Whether the Netty wire should be logged
    log.wire = true
    
    # The transport to use, either jdk or native.
    # Native socket transport has higher performance and produces less garbage but are only available on linux 
    transport = "native"
  }
}