{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim as gs\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This csv contains several irritating duplicates\n",
    "wordChain=pd.read_csv(\"./wordChains.csv\")\n",
    "wc={}\n",
    "count=0\n",
    "for i in wordChain.index:\n",
    "    w=wordChain.loc[i][\"Word\"].lower()\n",
    "    if w not in wc.keys():\n",
    "        wc[w]=wordChain.loc[i]\n",
    "        count+=1\n",
    "wordChain=pd.DataFrame(wc).transpose()[[\"PoS\",\"Theme\",\"Category\"]]\n",
    "G=nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vocab may not include all words in the csv since there may be some for which no word vectors exist\n",
    "vocab=wordChain.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for w1 in vocab:\n",
    "    for w2 in vocab:\n",
    "        if(w1[-1]==w2[0]):\n",
    "            G.add_edge(w1.lower(),w2.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['necklace', 'earring', 'garland', 'dress', 'socks'], 12),\n",
       " (['nails', 'stomach', 'hair', 'room', 'mirror'], 11),\n",
       " (['belt', 'turban', 'necklace', 'earring', 'garland'], 12),\n",
       " (['socks', 'slipper', 'ribbon', 'necklace', 'earring'], 12),\n",
       " (['turban', 'necklace', 'earring', 'garland', 'dress'], 12),\n",
       " (['rat', 'turtle', 'elephant', 'tiger', 'rabbit'], 12),\n",
       " (['air', 'rain', 'nails', 'stomach', 'hair'], 11),\n",
       " (['mirror', 'room', 'mug', 'glass', 'soap'], 12),\n",
       " (['turnip', 'pumpkin', 'nails', 'stomach', 'hair'], 11),\n",
       " (['turtle', 'elephant', 'tiger', 'rat', 'tap'], 11),\n",
       " (['roof', 'floor', 'room', 'mirror', 'radish'], 11),\n",
       " (['ant', 'turtle', 'elephant', 'tiger', 'rat'], 11),\n",
       " (['eraser', 'ruler', 'room', 'mirror', 'roof'], 11),\n",
       " (['floor', 'room', 'mirror', 'roof', 'fan'], 12),\n",
       " (['dress', 'socks', 'slipper', 'ribbon', 'necklace'], 12),\n",
       " (['bedroom', 'mirror', 'room', 'mug', 'glass'], 12),\n",
       " (['chair', 'room', 'mirror', 'roof', 'floor'], 11),\n",
       " (['slipper', 'ribbon', 'necklace', 'earring', 'garland'], 12)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_list=[]\n",
    "for z in range(1,100):\n",
    "    ls=[wordChain.index[np.random.randint(len(wordChain))]]\n",
    "    key_list=[\"PoS\",\"Theme\",\"Category\"]\n",
    "    total_score=0\n",
    "    for i in range(1,5):\n",
    "        max_score=0\n",
    "        w_next=\"\"\n",
    "        temp=G[ls[-1]]\n",
    "        for j in temp:\n",
    "            if(j in ls):\n",
    "                continue\n",
    "            obj1=wordChain.loc[ls[-1]]\n",
    "            obj2=wordChain.loc[j]\n",
    "            score=0\n",
    "            for key in key_list:\n",
    "                if(obj1[key]==obj2[key]):\n",
    "                    score+=1\n",
    "            if(score>max_score):\n",
    "                w_next=j\n",
    "                max_score=score\n",
    "        if(w_next!=\"\"):\n",
    "            ls.append(w_next)\n",
    "            total_score+=max_score\n",
    "    if(total_score>10 and (ls,total_score) not in chain_list):\n",
    "        chain_list.append((ls,total_score))\n",
    "chain_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors={}\n",
    "f=open(\"word_vectors.txt\",\"r\")\n",
    "for line in f.readlines():\n",
    "    word= line.split(\" \")[0]\n",
    "    vector= line.split(\" \")[1]\n",
    "    if(word.lower() in vocab):\n",
    "        word_vectors[word]=np.array([float(i) for i in vector.split(\",\")])\n",
    "f.close()\n",
    "len(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in word_vectors.keys():\n",
    "    word_vectors[key]/=np.linalg.norm(word_vectors[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#New vocab is subset of original vocab\n",
    "sem_vocab=word_vectors.keys()\n",
    "sem_G=G.subgraph(sem_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity(v1,v2):#Assuming normalised vectors (which has already been done above)\n",
    "    return (np.dot(np.transpose(v1),v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['lady', 'yellow', 'white', 'eraser', 'red'], 4.431834770083654),\n",
       " (['carry', 'yellow', 'white', 'eraser', 'red'], 4.1460875275599403),\n",
       " (['pumpkin', 'necklace', 'earring', 'guava', 'apple'], 5.4834985467841859),\n",
       " (['like', 'eat', 'think', 'know', 'want'], 5.2098665621137688),\n",
       " (['about', 'then', 'now', 'when', 'not'], 4.2134172556632006),\n",
       " (['know', 'want', 'think', 'knew', 'worry'], 5.686711951204579),\n",
       " (['out', 'together', 'room', 'mug', 'garden'], 3.1502800131930946),\n",
       " (['five', 'eight', 'three', 'eraser', 'radish'], 4.9243743980554306),\n",
       " (['cat', 'turtle', 'elephant', 'tiger', 'rabbit'], 6.3343513831439218),\n",
       " (['small', 'little', 'elephant', 'turtle', 'eagle'], 3.8950744532175401),\n",
       " (['cow', 'watermelon', 'necklace', 'earring', 'guava'], 5.7617726243921918),\n",
       " (['read', 'done', 'eat', 'take', 'excited'], 4.0489615198917432),\n",
       " (['toes', 'stomach', 'hair', 'radish', 'hat'], 4.784298372068851),\n",
       " (['dry', 'yellow', 'wet', 'thin', 'new'], 4.4106132078707905),\n",
       " (['chair', 'room', 'mug', 'glass', 'sharpener'], 4.5409073315481194),\n",
       " (['sat', 'take', 'eat', 'tell', 'let'], 4.0538318625696288),\n",
       " (['an', 'no', 'or', 'radish', 'how'], 3.1678009801246536),\n",
       " (['work', 'know', 'want', 'think', 'knew'], 4.9881468487160667),\n",
       " (['sari', 'india', 'apple', 'eraser', 'radish'], 4.4765136538809731),\n",
       " (['ruler', 'rat', 'tiger', 'rabbit', 'turtle'], 4.9051405324850457),\n",
       " (['soap', 'papaya', 'apple', 'eraser', 'radish'], 5.4655938169720688),\n",
       " (['could', 'did', 'does', 'shall', 'let'], 5.1202606903303156),\n",
       " (['right', 'too', 'only', 'yes', 'so'], 3.7422331239437683),\n",
       " (['keep', 'please', 'eat', 'think', 'know'], 4.3569745761761194),\n",
       " (['doll', 'lemon', 'necklace', 'earring', 'guava'], 5.3558543530936671),\n",
       " (['eat', 'take', 'eraser', 'radish', 'hen'], 4.2738329980995031),\n",
       " (['kind', 'dustbin', 'necklace', 'eraser', 'radish'], 4.9839670943679986)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_list=[]\n",
    "threshold_score=2\n",
    "for z in range(1,30):\n",
    "    ls=[sem_vocab[np.random.randint(len(sem_vocab))]]\n",
    "    total_score=0\n",
    "    for i in range(1,5):\n",
    "        max_score=0\n",
    "        w_next=\"\"\n",
    "        temp=sem_G[ls[-1]]\n",
    "        for j in temp:\n",
    "            score=0\n",
    "            if(j in ls):\n",
    "                continue\n",
    "            for word_ch in ls:\n",
    "                score+=similarity(word_vectors[word_ch],word_vectors[j])\n",
    "            if(score>max_score):\n",
    "                w_next=j\n",
    "                max_score=score\n",
    "        if(w_next!=\"\"):\n",
    "            ls.append(w_next)\n",
    "            total_score+=max_score\n",
    "    if(total_score>threshold_score and (ls,total_score) not in chain_list):\n",
    "        chain_list.append((ls,total_score))\n",
    "chain_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
