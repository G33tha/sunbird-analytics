{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual Document Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim as gs\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all the files in a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_files(directory):\n",
    "    filenames=[]\n",
    "    for root, dirs, files in os.walk(directory, topdown=False):\n",
    "        for name in files:\n",
    "            filenames.append(os.path.join(root,name))\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_file(filename):\n",
    "    try:\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        f=open(filename,\"r\")\n",
    "        temp=f.read().decode('unicode_escape').encode('ascii','ignore')\n",
    "        f.close()\n",
    "        intermediate = tokenizer.tokenize(temp)\n",
    "        stop = stopwords.words('english')\n",
    "        misc=['i','and','the', 'a', 'an']\n",
    "        word_list = [i.lower() for i in intermediate if \n",
    "                     ((i not in stop) and (i not in misc))]\n",
    "        return word_list\n",
    "    except:\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the documents into a suitable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_documents(directory):\n",
    "    filenames=get_files(directory)\n",
    "    doc=[]\n",
    "    for filename in filenames:\n",
    "        word_list=process_file(filename)\n",
    "        if(word_list!=None):\n",
    "            doc.append(gs.models.doc2vec.TaggedDocument(\n",
    "                    words=word_list,tags=[filename]))  \n",
    "        else:\n",
    "            print(filename+\" failed to load\")\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model basis these documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(directory):\n",
    "    doc=load_documents(directory)\n",
    "    model=gs.models.doc2vec.Doc2Vec(doc, size=100, window=8, \n",
    "                                    min_count=5, workers=4)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./masc_500k_texts/written/email/174124.txt~ failed to load\n",
      "./masc_500k_texts/written/email/175841.txt~ failed to load\n",
      "./masc_500k_texts/written/email/173906.txt~ failed to load\n",
      "./masc_500k_texts/written/email/175816.txt~ failed to load\n",
      "./masc_500k_texts/written/email/234267.txt~ failed to load\n",
      "./masc_500k_texts/written/email/175448.txt~ failed to load\n",
      "./masc_500k_texts/written/email/173252.txt~ failed to load\n",
      "./masc_500k_texts/written/email/176581.txt~ failed to load\n"
     ]
    }
   ],
   "source": [
    "model=train_model(\"./masc_500k_texts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the most similar documents in the corpus trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def most_similar_docs(filename):\n",
    "    print(\"Name:%s\\n\\nNearest documents:\\n\"%(filename))\n",
    "    for i in model.docvecs.most_similar(filename):\n",
    "        print(\"Name:%s\\nSimilarity:%s\\n\"%(i[0],i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:./masc_500k_texts/written/newspaper:newswire/20000410_nyt-NEW.txt\n",
      "\n",
      "Nearest documents:\n",
      "\n",
      "Name:./masc_500k_texts/written/newspaper:newswire/20000419_apw_eng-NEW.txt\n",
      "Similarity:0.843858599663\n",
      "\n",
      "Name:./masc_500k_texts/written/newspaper:newswire/20000415_apw_eng-NEW.txt\n",
      "Similarity:0.820624470711\n",
      "\n",
      "Name:./masc_500k_texts/written/newspaper:newswire/wsj_1640.mrg-NEW.txt\n",
      "Similarity:0.81544983387\n",
      "\n",
      "Name:./masc_500k_texts/written/newspaper:newswire/wsj_2465.txt\n",
      "Similarity:0.795952379704\n",
      "\n",
      "Name:./masc_500k_texts/written/newspaper:newswire/20000424_nyt-NEW.txt\n",
      "Similarity:0.788309693336\n",
      "\n",
      "Name:./masc_500k_texts/written/govt-docs/chapter-10.txt\n",
      "Similarity:0.787995398045\n",
      "\n",
      "Name:./masc_500k_texts/written/newspaper:newswire/NYTnewswire3.txt\n",
      "Similarity:0.754826664925\n",
      "\n",
      "Name:./masc_500k_texts/written/newspaper:newswire/wsj_0027.txt\n",
      "Similarity:0.702040672302\n",
      "\n",
      "Name:./masc_500k_texts/written/newspaper:newswire/NYTnewswire7.txt\n",
      "Similarity:0.687466025352\n",
      "\n",
      "Name:./masc_500k_texts/written/newspaper:newswire/20020731-nyt.txt\n",
      "Similarity:0.68726503849\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_similar_docs(\"./masc_500k_texts/written/newspaper:newswire/20000410_nyt-NEW.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer trained vectors on an external corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def infer_text_vectors(directory,model):\n",
    "    filenames=get_files(directory)\n",
    "    doc=[]\n",
    "    text_dict={}\n",
    "    for filename in filenames:\n",
    "        f=open(filename,\"r\")\n",
    "        string=f.read()\n",
    "        text_dict[filename]=model.infer_vector(string)\n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find nearest document in a corpus charcterised by a dictionary[document,vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_nearest_document(vector_dict,string,model):\n",
    "    x=model.infer_vector(string);\n",
    "    min_norm=1e8\n",
    "    nearest_file=\"\"\n",
    "    for i in vector_dict.keys():\n",
    "        temp=np.linalg.norm(vector_dict[i]-x)\n",
    "        if(temp<min_norm):\n",
    "            min_norm=temp\n",
    "            nearest_file=i\n",
    "    print(min_norm,nearest_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From here:\n",
    "#### 1. Given a training corpus and an external corpus, a model can be trained on the training corpus and used to infer the document vectors of the external corpus (keeping word vector representations the same), thus allowing us to cluster documents based on vector norms\n",
    "#### 2. We can also use train another model on the ekstep corpus and use it to find the nearest document in that corpus as well"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
